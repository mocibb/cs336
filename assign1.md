# 作业一参考资料
## 分词(Tokenizer)
分词指标包括，词汇数目(vocabulary size)和压缩率(compression ratio)

压缩率 = 字节数/token数 

常见算法包括: BPE，Unigram和WordPiece。

- [openai](https://platform.openai.com/tokenizer)
- [tiktokenizer比较](https://tiktokenizer.vercel.app/)
- [deepseek](https://lunary.ai/deepseek-tokenizer)

词汇大小
<img src="https://github.com/user-attachments/assets/4526866f-c433-4f4b-8e66-da5c7e25b8f6" alt="vocab" width="600"/>


## 模型
### RMSNorm

### SwiGLU


### Serial vs Parallel layers

### RoPE

### Hyperparameters

### Dropout and other regularization

### GQA/MQA

### The Full Transformer LM资源计算表
[计算器](https://docs.google.com/spreadsheets/d/1LebxBI5lkoNdMFEBIOIEnHylSvvzoC8xvWMBcXwjy7U/edit?usp=sharing)

## 训练

## 实验
