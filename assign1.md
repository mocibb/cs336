# 作业一参考资料
## 分词(Tokenizer)
分词指标包括，词汇数目(vocabulary size)和压缩率(compression ratio)

压缩率 = 字节数/token数 

常见算法包括: BPE和bert分词

- [openai](https://platform.openai.com/tokenizer)
- [tiktokenizer比较](https://tiktokenizer.vercel.app/)
- [deepseek](https://lunary.ai/deepseek-tokenizer)


## 模型
### RMSNorm

### SwiGLU


### Serial vs Parallel layers

### RoPE

### Hyperparameters

### Dropout and other regularization

### GQA/MQA

## The Full Transformer LM资源计算表
[计算器](https://docs.google.com/spreadsheets/d/1LebxBI5lkoNdMFEBIOIEnHylSvvzoC8xvWMBcXwjy7U/edit?usp=sharing)
